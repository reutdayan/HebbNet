{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reutdayan/HebbNet/blob/main/HebbNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRQqvQa9uktb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive - to save weights in google drive"
      ],
      "metadata": {
        "id": "lFfVCZHEI4NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysMoFk1qI2kq",
        "outputId": "3896324a-d3fb-47c2-be1d-cebd768401b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAUSLHORvJLD"
      },
      "source": [
        "Download and upload Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCKkffXduoZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1628659a-205b-42da-a876-bfe07520c1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 74795023.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 35691129.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 19813298.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9045835.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN-P5_9FvOsk"
      },
      "source": [
        "Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Ydap7Ouz0h"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 1\n",
        "lr = 1 # η, the learning rate\n",
        "p = 0.01  # top-p percentile for gradient sparsification\n",
        "epochs = 200\n",
        "momentum = 5e-4\n",
        "lr_decay = 0.95\n",
        "\n",
        "input_layer_size = 28*28\n",
        "hidden_layer_size = 2000\n",
        "output_layer_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ6wP56GvAI9",
        "outputId": "3fcacc14-e6df-4b19-e822-47dc27c4f0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([1, 1, 28, 28])\n",
            "Shape of y: torch.Size([1]) torch.int64\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzM07EcRviS5"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class HebbNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.hebbian_weights = nn.Linear(input_layer_size, hidden_layer_size, False)\n",
        "        self.classification_weights = nn.Linear(hidden_layer_size, output_layer_size, True)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        z = self.hebbian_weights(x)\n",
        "        z = self.relu(z)  # Apply ReLU activation after the Hebbian layer\n",
        "        pred = self.classification_weights(z)\n",
        "        pred = self.softmax(pred)\n",
        "        return x,z,pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TELxmZ6dvofm"
      },
      "outputs": [],
      "source": [
        "class BatchedHebbRuleWithActivationThreshold(nn.Module):\n",
        "    def __init__(self, hidden_layer_size=2000, input_layer_size=784, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.register_buffer('w1_activation_thresholds', torch.zeros((hidden_layer_size, input_layer_size)))\n",
        "        self.t = 1\n",
        "\n",
        "    def forward(self, x_input: torch.Tensor, z_hidden: torch.Tensor):\n",
        "        '''\n",
        "        Args:\n",
        "        x_input: torch.Tensor of shape (input_layer): (784)\n",
        "        hidden: torch.Tensor of shape (hidden_layer): (2000)\n",
        "\n",
        "\n",
        "        Output:\n",
        "        shape: hidden_layerXinput_layer (2000,784)\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "          z = z_hidden[...,None] # (B,2000,1)\n",
        "          x = x_input[:,None] # (B,1,784)\n",
        "          activation = torch.einsum('bij,bjk->bik',z,x) #(B,2000,784) - matrix multipication\n",
        "\n",
        "          if self.t==1:\n",
        "              delta_w1 = activation\n",
        "          else:\n",
        "              delta_w1 = activation - (self.w1_activation_thresholds[None] / (self.t-1))\n",
        "\n",
        "          self.w1_activation_thresholds += activation.sum(0)\n",
        "          self.t = self.t + self.batch_size\n",
        "          return delta_w1.mean(0) # Mean over batch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HebbRuleWithActivationThreshold(nn.Module):\n",
        "    def __init__(self, hidden_layer_size=2000, input_layer_size=784):\n",
        "        super().__init__()\n",
        "        self.register_buffer('w1_activation_thresholds', torch.zeros((hidden_layer_size, input_layer_size)))\n",
        "        self.t = 1\n",
        "\n",
        "    def forward(self, x: torch.Tensor, z: torch.Tensor):\n",
        "        with torch.no_grad():\n",
        "          activation = torch.matmul(z.T,x) #(2000,784) - matrix multipication\n",
        "\n",
        "          if self.t==1:\n",
        "              delta_w1 = activation\n",
        "          else:\n",
        "              delta_w1 = activation - (self.w1_activation_thresholds / (self.t-1))\n",
        "\n",
        "          self.w1_activation_thresholds += activation\n",
        "          self.t += 1\n",
        "          return delta_w1"
      ],
      "metadata": {
        "id": "ZqR0-w1p24pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3HqOcRDy8d7"
      },
      "outputs": [],
      "source": [
        "def gradiant_sparsity(delta_w1, p):\n",
        "  # Calculate the number of values to keep based on the percentile (p)\n",
        "  num_values_to_keep = int( p * delta_w1.numel())\n",
        "\n",
        "  # Find the top k values and their indices\n",
        "  top_values, _ = torch.topk(torch.abs(delta_w1).view(-1), num_values_to_keep)\n",
        "  threshold = top_values[-1]  # The threshold is the k-th largest value\n",
        "\n",
        "  # Set values below the threshold to zero\n",
        "  delta_w1 = torch.where(torch.abs(delta_w1) >= threshold, delta_w1, torch.tensor(0.0).to(device))\n",
        "  return delta_w1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEOcUqIpvvJN"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, lr, activation_thresholder: HebbRuleWithActivationThreshold):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        X,z_hidden,pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # optimize classifiction wieghts\n",
        "        optimizer.step()\n",
        "\n",
        "        # optimize hebbian weights\n",
        "        # activation threshold\n",
        "        delta_w1 = activation_thresholder(X, z_hidden)\n",
        "\n",
        "        # Gradient sparsity\n",
        "        delta_w1 = gradiant_sparsity(delta_w1, p)\n",
        "\n",
        "        # update hebbian weights\n",
        "        model.hebbian_weights.weight.data = model.hebbian_weights.weight.data - lr*delta_w1\n",
        "\n",
        "        if batch % 10000 == 0:\n",
        "          loss, current = loss.item(), (batch + 1) * len(X)\n",
        "          print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    correct /= size\n",
        "    print(f\"Train Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UnFtL-cvz6g"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            x,z,pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return 100* correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhBZdLSzv3w9"
      },
      "outputs": [],
      "source": [
        "def visualize_weights (weights):\n",
        "    for neuron in weights:\n",
        "        image = neuron.reshape(28,28).cpu()\n",
        "        plt.imshow(image.detach().numpy())\n",
        "        plt.show()\n",
        "\n",
        "def visualize_activations(weights,X):\n",
        "  X,z_hidden,pred = model(X)\n",
        "  delta_w1 = activation_thresholder(X, z_hidden).squeeze()\n",
        "  plt.imshow(delta_w1.detach().numpy())\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, activation_thresholder, optimizer, scheduler, filepath):\n",
        "    \"\"\"\n",
        "    Save PyTorch model, activation_thresholder, optimizer, and scheduler to a file.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be saved.\n",
        "        activation_thresholder: The activation_thresholder object to be saved.\n",
        "        optimizer: The optimizer used for training the model.\n",
        "        scheduler: The learning rate scheduler.\n",
        "        filepath (str): The path to the file where the model and other objects will be saved.\n",
        "    \"\"\"\n",
        "    # Create a dictionary to store the model, activation_thresholder, optimizer, and scheduler states\n",
        "    state = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'activation_thresholder': activation_thresholder,\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
        "    }\n",
        "\n",
        "    # Save the state dictionary to the specified file using torch.save\n",
        "    torch.save(state, filepath)\n",
        "\n",
        "def load_model(model, optimizer, scheduler, filepath):\n",
        "    \"\"\"\n",
        "    Load PyTorch model, activation_thresholder, optimizer, and scheduler from a file.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be loaded.\n",
        "        optimizer: The optimizer to be loaded.\n",
        "        scheduler: The learning rate scheduler to be loaded.\n",
        "        filepath (str): The path to the file from which the objects will be loaded.\n",
        "\n",
        "    Returns:\n",
        "        model, activation_thresholder, optimizer, scheduler: The loaded model, activation_thresholder,\n",
        "        optimizer, and scheduler.\n",
        "    \"\"\"\n",
        "    # Load the saved state dictionary from the file using torch.load\n",
        "    state = torch.load(filepath)\n",
        "\n",
        "    # Load the model, activation_thresholder, and optimizer states\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    activation_thresholder = state['activation_thresholder']\n",
        "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "\n",
        "    # Load the scheduler state, if it exists\n",
        "    if scheduler and state['scheduler_state_dict']:\n",
        "        scheduler.load_state_dict(state['scheduler_state_dict'])\n",
        "\n",
        "    return model, activation_thresholder, optimizer, scheduler"
      ],
      "metadata": {
        "id": "tCHpvdlWG6jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzm13ENyvjm_",
        "outputId": "f00a2e75-5683-479e-a3ff-8277f435cb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (hebbian_weights): Linear(in_features=784, out_features=2000, bias=False)\n",
            "  (classification_weights): Linear(in_features=2000, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "Epoch 0\n",
            "-------------------------------\n",
            "loss: 2.356591  [    1/60000]\n",
            "loss: 2.207929  [10001/60000]\n",
            "loss: 2.090814  [20001/60000]\n",
            "loss: 37446008.000000  [30001/60000]\n",
            "loss: 3.105266  [40001/60000]\n",
            "loss: 2.914796  [50001/60000]\n",
            "Train Accuracy: 9.5%\n",
            "Test Error: \n",
            " Accuracy: 11.9%, Avg loss: 8269772064.914073 \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 541.339111  [    1/60000]\n",
            "loss: 2.551543  [10001/60000]\n",
            "loss: 2.126985  [20001/60000]\n",
            "loss: 2.478526  [30001/60000]\n",
            "loss: 4.136813  [40001/60000]\n",
            "loss: 2.949136  [50001/60000]\n",
            "Train Accuracy: 9.2%\n",
            "Test Error: \n",
            " Accuracy: 12.0%, Avg loss: 7497279728.182299 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 3.323491  [    1/60000]\n",
            "loss: 2.540766  [10001/60000]\n",
            "loss: 2.196618  [20001/60000]\n",
            "loss: 18434702.000000  [30001/60000]\n",
            "loss: 3.099836  [40001/60000]\n",
            "loss: 2.735676  [50001/60000]\n",
            "Train Accuracy: 9.1%\n",
            "Test Error: \n",
            " Accuracy: 10.9%, Avg loss: 19350359948.356514 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 3.378503  [    1/60000]\n",
            "loss: 2.445589  [10001/60000]\n",
            "loss: 1.906348  [20001/60000]\n",
            "loss: 14161917952.000000  [30001/60000]\n",
            "loss: 3.335651  [40001/60000]\n",
            "loss: 2.784187  [50001/60000]\n",
            "Train Accuracy: 8.9%\n",
            "Test Error: \n",
            " Accuracy: 10.7%, Avg loss: 18334696937.634304 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.610779  [    1/60000]\n",
            "loss: 2.555839  [10001/60000]\n",
            "loss: 1.805446  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 3.192157  [40001/60000]\n",
            "loss: 2.756252  [50001/60000]\n",
            "Train Accuracy: 9.0%\n",
            "Test Error: \n",
            " Accuracy: 11.1%, Avg loss: 21690242789.323719 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.105830  [    1/60000]\n",
            "loss: 2.639757  [10001/60000]\n",
            "loss: 2.166357  [20001/60000]\n",
            "loss: 2.783021  [30001/60000]\n",
            "loss: 3.202963  [40001/60000]\n",
            "loss: 2.848301  [50001/60000]\n",
            "Train Accuracy: 9.0%\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 29052752927.393261 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.988902  [    1/60000]\n",
            "loss: 2.526757  [10001/60000]\n",
            "loss: 1.899746  [20001/60000]\n",
            "loss: 94251.210938  [30001/60000]\n",
            "loss: 3.054813  [40001/60000]\n",
            "loss: 2.578695  [50001/60000]\n",
            "Train Accuracy: 9.0%\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 21301321044.073204 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.077131  [    1/60000]\n",
            "loss: 3.036825  [10001/60000]\n",
            "loss: 1.838440  [20001/60000]\n",
            "loss: 2.830926  [30001/60000]\n",
            "loss: 3.241406  [40001/60000]\n",
            "loss: 2.639098  [50001/60000]\n",
            "Train Accuracy: 8.9%\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 19300931266.087719 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.868884  [    1/60000]\n",
            "loss: 2.479618  [10001/60000]\n",
            "loss: 1.814752  [20001/60000]\n",
            "loss: 2.523009  [30001/60000]\n",
            "loss: 3.654613  [40001/60000]\n",
            "loss: 2.601167  [50001/60000]\n",
            "Train Accuracy: 9.0%\n",
            "Test Error: \n",
            " Accuracy: 10.7%, Avg loss: 23918707421.985298 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.543135  [    1/60000]\n",
            "loss: 2.505666  [10001/60000]\n",
            "loss: 1.902035  [20001/60000]\n",
            "loss: 2.558512  [30001/60000]\n",
            "loss: 2.967863  [40001/60000]\n",
            "loss: 2.588131  [50001/60000]\n",
            "Train Accuracy: 8.9%\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 19131492700.229103 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 3.459530  [    1/60000]\n",
            "loss: 2.463663  [10001/60000]\n",
            "loss: 1.835869  [20001/60000]\n",
            "loss: 2.907666  [30001/60000]\n",
            "loss: 3.014822  [40001/60000]\n",
            "loss: 2.555955  [50001/60000]\n",
            "Train Accuracy: 9.1%\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 20033242272.421780 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.820621  [    1/60000]\n",
            "loss: 2.481448  [10001/60000]\n",
            "loss: 1.969785  [20001/60000]\n",
            "loss: 2.962220  [30001/60000]\n",
            "loss: 3.237063  [40001/60000]\n",
            "loss: 2.535473  [50001/60000]\n",
            "Train Accuracy: 9.0%\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 9272774774.575226 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.451744  [    1/60000]\n",
            "loss: 2.388596  [10001/60000]\n",
            "loss: 1.908932  [20001/60000]\n",
            "loss: 2.551888  [30001/60000]\n",
            "loss: 2.997291  [40001/60000]\n",
            "loss: 2.517428  [50001/60000]\n",
            "Train Accuracy: 8.9%\n",
            "Test Error: \n",
            " Accuracy: 10.6%, Avg loss: 105929292192.325943 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.129390  [    1/60000]\n",
            "loss: 2.464734  [10001/60000]\n",
            "loss: 1.895169  [20001/60000]\n",
            "loss: 1992631552.000000  [30001/60000]\n",
            "loss: 3.136291  [40001/60000]\n",
            "loss: 2.447805  [50001/60000]\n",
            "Train Accuracy: 9.0%\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 26336840560.890991 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.883281  [    1/60000]\n",
            "loss: 2.397626  [10001/60000]\n",
            "loss: 1.926841  [20001/60000]\n",
            "loss: 2.865934  [30001/60000]\n",
            "loss: 2.919035  [40001/60000]\n",
            "loss: 2.505753  [50001/60000]\n",
            "Train Accuracy: 9.1%\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 8970192542.119034 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.377809  [    1/60000]\n",
            "loss: 2.460678  [10001/60000]\n",
            "loss: 1.955587  [20001/60000]\n",
            "loss: 2.549555  [30001/60000]\n",
            "loss: 2.954049  [40001/60000]\n",
            "loss: 2.446008  [50001/60000]\n",
            "Train Accuracy: 9.1%\n",
            "Test Error: \n",
            " Accuracy: 10.2%, Avg loss: 42780250933.069710 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.137601  [    1/60000]\n",
            "loss: 2.473017  [10001/60000]\n",
            "loss: 2.216306  [20001/60000]\n",
            "loss: 2.559581  [30001/60000]\n",
            "loss: 2.941264  [40001/60000]\n",
            "loss: 2.751738  [50001/60000]\n",
            "Train Accuracy: 9.2%\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 15845829598.731718 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.528933  [    1/60000]\n",
            "loss: 2.400374  [10001/60000]\n",
            "loss: 2.248817  [20001/60000]\n",
            "loss: 3.041813  [30001/60000]\n",
            "loss: 3.007668  [40001/60000]\n",
            "loss: 2.391472  [50001/60000]\n",
            "Train Accuracy: 9.2%\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 13182992149.158213 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.546284  [    1/60000]\n",
            "loss: 2.448335  [10001/60000]\n",
            "loss: 1.979390  [20001/60000]\n",
            "loss: 2.431311  [30001/60000]\n",
            "loss: 2.988677  [40001/60000]\n",
            "loss: 2.306938  [50001/60000]\n",
            "Train Accuracy: 9.3%\n",
            "Test Error: \n",
            " Accuracy: 10.9%, Avg loss: 16689166275.051962 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.599102  [    1/60000]\n",
            "loss: 2.348596  [10001/60000]\n",
            "loss: 2.169402  [20001/60000]\n",
            "loss: 2.370444  [30001/60000]\n",
            "loss: 3.553917  [40001/60000]\n",
            "loss: 2.414213  [50001/60000]\n",
            "Train Accuracy: 9.1%\n",
            "Test Error: \n",
            " Accuracy: 10.6%, Avg loss: 12227250825.416565 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.800095  [    1/60000]\n",
            "loss: 2.399302  [10001/60000]\n",
            "loss: 1.991307  [20001/60000]\n",
            "loss: 2.433136  [30001/60000]\n",
            "loss: 2.878641  [40001/60000]\n",
            "loss: 2.315050  [50001/60000]\n",
            "Train Accuracy: 9.3%\n",
            "Test Error: \n",
            " Accuracy: 11.9%, Avg loss: 9282936441.066105 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 2.537839  [    1/60000]\n",
            "loss: 2.525928  [10001/60000]\n",
            "loss: 1.937912  [20001/60000]\n",
            "loss: 2.381318  [30001/60000]\n",
            "loss: 2.864404  [40001/60000]\n",
            "loss: 2.614969  [50001/60000]\n",
            "Train Accuracy: 9.3%\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 23214000206.768318 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 2.783004  [    1/60000]\n",
            "loss: 2.385735  [10001/60000]\n",
            "loss: 2.237458  [20001/60000]\n",
            "loss: 2.377660  [30001/60000]\n",
            "loss: 3.175820  [40001/60000]\n",
            "loss: 2.302380  [50001/60000]\n",
            "Train Accuracy: 9.3%\n",
            "Test Error: \n",
            " Accuracy: 10.7%, Avg loss: 13813988638.371464 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 2.689826  [    1/60000]\n",
            "loss: 2.511288  [10001/60000]\n",
            "loss: 1.948806  [20001/60000]\n",
            "loss: 2.410568  [30001/60000]\n",
            "loss: 2.743529  [40001/60000]\n",
            "loss: 2.274159  [50001/60000]\n",
            "Train Accuracy: 9.5%\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 7506269637.729424 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 3.040531  [    1/60000]\n",
            "loss: 2.484971  [10001/60000]\n",
            "loss: 2.059896  [20001/60000]\n",
            "loss: 3.034056  [30001/60000]\n",
            "loss: 3.006184  [40001/60000]\n",
            "loss: 2.281223  [50001/60000]\n",
            "Train Accuracy: 9.4%\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 28351275456.280132 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 2.301605  [    1/60000]\n",
            "loss: 2.325646  [10001/60000]\n",
            "loss: 2.051573  [20001/60000]\n",
            "loss: 212472266752.000000  [30001/60000]\n",
            "loss: 2.853255  [40001/60000]\n",
            "loss: 2.212158  [50001/60000]\n",
            "Train Accuracy: 9.6%\n",
            "Test Error: \n",
            " Accuracy: 11.1%, Avg loss: 143975344779.753540 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 2.410770  [    1/60000]\n",
            "loss: 2.454694  [10001/60000]\n",
            "loss: 1.928777  [20001/60000]\n",
            "loss: 2.622445  [30001/60000]\n",
            "loss: 2.950740  [40001/60000]\n",
            "loss: 2.166313  [50001/60000]\n",
            "Train Accuracy: 9.7%\n",
            "Test Error: \n",
            " Accuracy: 11.4%, Avg loss: 8636841364.460115 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 2.622102  [    1/60000]\n",
            "loss: 2.374148  [10001/60000]\n",
            "loss: 2.134122  [20001/60000]\n",
            "loss: 2.345307  [30001/60000]\n",
            "loss: 2.723827  [40001/60000]\n",
            "loss: 2.252919  [50001/60000]\n",
            "Train Accuracy: 9.4%\n",
            "Test Error: \n",
            " Accuracy: 12.1%, Avg loss: 37184787961.723396 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 2.858223  [    1/60000]\n",
            "loss: 2.359169  [10001/60000]\n",
            "loss: 2.104864  [20001/60000]\n",
            "loss: 2.600908  [30001/60000]\n",
            "loss: 2.846306  [40001/60000]\n",
            "loss: 2.225041  [50001/60000]\n",
            "Train Accuracy: 9.6%\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 36734369428.575447 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 2.523376  [    1/60000]\n",
            "loss: 2.265513  [10001/60000]\n",
            "loss: 2.138106  [20001/60000]\n",
            "loss: 265777408.000000  [30001/60000]\n",
            "loss: 2.710117  [40001/60000]\n",
            "loss: 2.271842  [50001/60000]\n",
            "Train Accuracy: 9.8%\n",
            "Test Error: \n",
            " Accuracy: 11.0%, Avg loss: 11414664944.548061 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 2.716425  [    1/60000]\n",
            "loss: 2.430558  [10001/60000]\n",
            "loss: 2.191079  [20001/60000]\n",
            "loss: 2.556168  [30001/60000]\n",
            "loss: 3.021823  [40001/60000]\n",
            "loss: 2.350266  [50001/60000]\n",
            "Train Accuracy: 9.9%\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 4837932621.570661 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 2.588627  [    1/60000]\n",
            "loss: 2.416705  [10001/60000]\n",
            "loss: 1.967231  [20001/60000]\n",
            "loss: 2.749160  [30001/60000]\n",
            "loss: 2.647033  [40001/60000]\n",
            "loss: 2.248597  [50001/60000]\n",
            "Train Accuracy: 9.8%\n",
            "Test Error: \n",
            " Accuracy: 11.3%, Avg loss: 5696974929.833758 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 2.459931  [    1/60000]\n",
            "loss: 2.283896  [10001/60000]\n",
            "loss: 1.943300  [20001/60000]\n",
            "loss: 2.491407  [30001/60000]\n",
            "loss: 3.280789  [40001/60000]\n",
            "loss: 2.182165  [50001/60000]\n",
            "Train Accuracy: 9.9%\n",
            "Test Error: \n",
            " Accuracy: 11.0%, Avg loss: 6266465054.814285 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 2.381740  [    1/60000]\n",
            "loss: 2.226661  [10001/60000]\n",
            "loss: 2.051120  [20001/60000]\n",
            "loss: 184585248.000000  [30001/60000]\n",
            "loss: 2.713481  [40001/60000]\n",
            "loss: 2.164446  [50001/60000]\n",
            "Train Accuracy: 9.8%\n",
            "Test Error: \n",
            " Accuracy: 11.4%, Avg loss: 4953524933.890532 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 3.149786  [    1/60000]\n",
            "loss: 2.557477  [10001/60000]\n",
            "loss: 2.112086  [20001/60000]\n",
            "loss: 18185838592.000000  [30001/60000]\n",
            "loss: 2.755488  [40001/60000]\n",
            "loss: 2.126094  [50001/60000]\n",
            "Train Accuracy: 10.2%\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 3010623432.526357 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 2.492651  [    1/60000]\n",
            "loss: 2.243238  [10001/60000]\n",
            "loss: 1.965074  [20001/60000]\n",
            "loss: 7382071296.000000  [30001/60000]\n",
            "loss: 2.779846  [40001/60000]\n",
            "loss: 2.179004  [50001/60000]\n",
            "Train Accuracy: 10.2%\n",
            "Test Error: \n",
            " Accuracy: 11.7%, Avg loss: 6981430284.342050 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 3.029937  [    1/60000]\n",
            "loss: 2.397519  [10001/60000]\n",
            "loss: 1.980742  [20001/60000]\n",
            "loss: 507940.000000  [30001/60000]\n",
            "loss: 2.838603  [40001/60000]\n",
            "loss: 2.154387  [50001/60000]\n",
            "Train Accuracy: 10.3%\n",
            "Test Error: \n",
            " Accuracy: 11.6%, Avg loss: 4133787335.819747 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 2.498488  [    1/60000]\n",
            "loss: 2.592393  [10001/60000]\n",
            "loss: 2.150704  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.689988  [40001/60000]\n",
            "loss: 2.129204  [50001/60000]\n",
            "Train Accuracy: 10.3%\n",
            "Test Error: \n",
            " Accuracy: 11.7%, Avg loss: 3541529187.853580 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 2.378501  [    1/60000]\n",
            "loss: 2.766447  [10001/60000]\n",
            "loss: 1.994400  [20001/60000]\n",
            "loss: 2.565021  [30001/60000]\n",
            "loss: 42480181248.000000  [40001/60000]\n",
            "loss: 2.232570  [50001/60000]\n",
            "Train Accuracy: 10.4%\n",
            "Test Error: \n",
            " Accuracy: 11.4%, Avg loss: 6172308380.421515 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 3.222380  [    1/60000]\n",
            "loss: 2.265093  [10001/60000]\n",
            "loss: 2.103495  [20001/60000]\n",
            "loss: 2.426066  [30001/60000]\n",
            "loss: 2.581674  [40001/60000]\n",
            "loss: 2.276757  [50001/60000]\n",
            "Train Accuracy: 10.4%\n",
            "Test Error: \n",
            " Accuracy: 12.1%, Avg loss: 4261899141.113608 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 2.743366  [    1/60000]\n",
            "loss: 2.342644  [10001/60000]\n",
            "loss: 2.103548  [20001/60000]\n",
            "loss: 2.219822  [30001/60000]\n",
            "loss: 3.009302  [40001/60000]\n",
            "loss: 2.163038  [50001/60000]\n",
            "Train Accuracy: 10.7%\n",
            "Test Error: \n",
            " Accuracy: 12.1%, Avg loss: 3877230483.078151 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 2.891076  [    1/60000]\n",
            "loss: 2.345520  [10001/60000]\n",
            "loss: 1.928813  [20001/60000]\n",
            "loss: 2.408290  [30001/60000]\n",
            "loss: 2.716888  [40001/60000]\n",
            "loss: 2.086384  [50001/60000]\n",
            "Train Accuracy: 10.7%\n",
            "Test Error: \n",
            " Accuracy: 12.2%, Avg loss: 7342539338.404472 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 2.393091  [    1/60000]\n",
            "loss: 2.221984  [10001/60000]\n",
            "loss: 2.153983  [20001/60000]\n",
            "loss: 2.617143  [30001/60000]\n",
            "loss: 2.783979  [40001/60000]\n",
            "loss: 2.187585  [50001/60000]\n",
            "Train Accuracy: 11.0%\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 3340761042.032813 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 2.885781  [    1/60000]\n",
            "loss: 2.234702  [10001/60000]\n",
            "loss: 2.081341  [20001/60000]\n",
            "loss: 4881694720.000000  [30001/60000]\n",
            "loss: 2.898955  [40001/60000]\n",
            "loss: 2.269132  [50001/60000]\n",
            "Train Accuracy: 11.1%\n",
            "Test Error: \n",
            " Accuracy: 11.9%, Avg loss: 3267477436.812534 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 3.128776  [    1/60000]\n",
            "loss: 2.352649  [10001/60000]\n",
            "loss: 2.063520  [20001/60000]\n",
            "loss: 2.533437  [30001/60000]\n",
            "loss: 2.583752  [40001/60000]\n",
            "loss: 2.071373  [50001/60000]\n",
            "Train Accuracy: 11.3%\n",
            "Test Error: \n",
            " Accuracy: 12.5%, Avg loss: 2408249571.084519 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 2.937978  [    1/60000]\n",
            "loss: 2.256694  [10001/60000]\n",
            "loss: 2.118008  [20001/60000]\n",
            "loss: 2.477978  [30001/60000]\n",
            "loss: 2.652444  [40001/60000]\n",
            "loss: 2.115004  [50001/60000]\n",
            "Train Accuracy: 11.1%\n",
            "Test Error: \n",
            " Accuracy: 13.0%, Avg loss: 2294645325.171232 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 11.157676  [    1/60000]\n",
            "loss: 2.207436  [10001/60000]\n",
            "loss: 2.068953  [20001/60000]\n",
            "loss: 2055700480.000000  [30001/60000]\n",
            "loss: 2.620020  [40001/60000]\n",
            "loss: 2.143005  [50001/60000]\n",
            "Train Accuracy: 11.2%\n",
            "Test Error: \n",
            " Accuracy: 11.5%, Avg loss: 2586755355.468985 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 2.850367  [    1/60000]\n",
            "loss: 2.311591  [10001/60000]\n",
            "loss: 2.077437  [20001/60000]\n",
            "loss: 2.523981  [30001/60000]\n",
            "loss: 2.781644  [40001/60000]\n",
            "loss: 2.158275  [50001/60000]\n",
            "Train Accuracy: 11.5%\n",
            "Test Error: \n",
            " Accuracy: 12.4%, Avg loss: 3192850274.348584 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 3.031446  [    1/60000]\n",
            "loss: 2.278598  [10001/60000]\n",
            "loss: 2.194296  [20001/60000]\n",
            "loss: 2.545122  [30001/60000]\n",
            "loss: 2.884806  [40001/60000]\n",
            "loss: 2.253550  [50001/60000]\n",
            "Train Accuracy: 11.6%\n",
            "Test Error: \n",
            " Accuracy: 13.5%, Avg loss: 4453360495.149565 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 123.923828  [    1/60000]\n",
            "loss: 2.476867  [10001/60000]\n",
            "loss: 2.240839  [20001/60000]\n",
            "loss: 14705984.000000  [30001/60000]\n",
            "loss: 2.692329  [40001/60000]\n",
            "loss: 2.260981  [50001/60000]\n",
            "Train Accuracy: 11.7%\n",
            "Test Error: \n",
            " Accuracy: 12.9%, Avg loss: 4824648453.793723 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 2.791426  [    1/60000]\n",
            "loss: 2.260664  [10001/60000]\n",
            "loss: 2.095345  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 3.174495  [40001/60000]\n",
            "loss: 2.134222  [50001/60000]\n",
            "Train Accuracy: 12.1%\n",
            "Test Error: \n",
            " Accuracy: 13.3%, Avg loss: 3613803451.385862 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.352564  [10001/60000]\n",
            "loss: 2.099946  [20001/60000]\n",
            "loss: 1175240192.000000  [30001/60000]\n",
            "loss: 2.777013  [40001/60000]\n",
            "loss: 2.127276  [50001/60000]\n",
            "Train Accuracy: 12.2%\n",
            "Test Error: \n",
            " Accuracy: 14.2%, Avg loss: 5011897975.582996 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.334178  [10001/60000]\n",
            "loss: 2.001376  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.808574  [40001/60000]\n",
            "loss: 2.229170  [50001/60000]\n",
            "Train Accuracy: 12.4%\n",
            "Test Error: \n",
            " Accuracy: 13.8%, Avg loss: 6737234498.663992 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 170.206543  [    1/60000]\n",
            "loss: 2.351164  [10001/60000]\n",
            "loss: 2.186701  [20001/60000]\n",
            "loss: 37204.000000  [30001/60000]\n",
            "loss: 2.942454  [40001/60000]\n",
            "loss: 2.004628  [50001/60000]\n",
            "Train Accuracy: 12.7%\n",
            "Test Error: \n",
            " Accuracy: 14.2%, Avg loss: 4488017104.296544 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 2.663743  [    1/60000]\n",
            "loss: 2.239274  [10001/60000]\n",
            "loss: 2.265407  [20001/60000]\n",
            "loss: 2.433238  [30001/60000]\n",
            "loss: 2.997316  [40001/60000]\n",
            "loss: 2.225722  [50001/60000]\n",
            "Train Accuracy: 13.0%\n",
            "Test Error: \n",
            " Accuracy: 14.7%, Avg loss: 5142834965.164008 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 2.859334  [    1/60000]\n",
            "loss: 2.138000  [10001/60000]\n",
            "loss: 2.207331  [20001/60000]\n",
            "loss: 2.436030  [30001/60000]\n",
            "loss: 3.023107  [40001/60000]\n",
            "loss: 1.979616  [50001/60000]\n",
            "Train Accuracy: 12.9%\n",
            "Test Error: \n",
            " Accuracy: 13.9%, Avg loss: 5107480555.647611 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 16189821.000000  [    1/60000]\n",
            "loss: 2.244477  [10001/60000]\n",
            "loss: 2.330251  [20001/60000]\n",
            "loss: 2577760.000000  [30001/60000]\n",
            "loss: 2.822547  [40001/60000]\n",
            "loss: 2.195747  [50001/60000]\n",
            "Train Accuracy: 13.0%\n",
            "Test Error: \n",
            " Accuracy: 15.0%, Avg loss: 4971142172.913991 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 2.827010  [    1/60000]\n",
            "loss: 2.122116  [10001/60000]\n",
            "loss: 2.159013  [20001/60000]\n",
            "loss: 93.147446  [30001/60000]\n",
            "loss: 2.623479  [40001/60000]\n",
            "loss: 2.080549  [50001/60000]\n",
            "Train Accuracy: 13.5%\n",
            "Test Error: \n",
            " Accuracy: 14.7%, Avg loss: 5181719027.922700 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 36.344391  [    1/60000]\n",
            "loss: 2.172745  [10001/60000]\n",
            "loss: 2.259844  [20001/60000]\n",
            "loss: 2.234946  [30001/60000]\n",
            "loss: 2.938453  [40001/60000]\n",
            "loss: 1.995318  [50001/60000]\n",
            "Train Accuracy: 13.7%\n",
            "Test Error: \n",
            " Accuracy: 14.2%, Avg loss: 5296520252.698136 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 109.190186  [    1/60000]\n",
            "loss: 2.039593  [10001/60000]\n",
            "loss: 2.129569  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.892824  [40001/60000]\n",
            "loss: 1.967503  [50001/60000]\n",
            "Train Accuracy: 14.1%\n",
            "Test Error: \n",
            " Accuracy: 15.7%, Avg loss: 6276503339.892222 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 2.791384  [    1/60000]\n",
            "loss: 2.044790  [10001/60000]\n",
            "loss: 2.196114  [20001/60000]\n",
            "loss: 6477720.000000  [30001/60000]\n",
            "loss: 2.665443  [40001/60000]\n",
            "loss: 2.027957  [50001/60000]\n",
            "Train Accuracy: 14.3%\n",
            "Test Error: \n",
            " Accuracy: 15.2%, Avg loss: 6369623502.903888 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 2.610798  [    1/60000]\n",
            "loss: 2.207664  [10001/60000]\n",
            "loss: 2.206236  [20001/60000]\n",
            "loss: 2.523395  [30001/60000]\n",
            "loss: 2.801119  [40001/60000]\n",
            "loss: 2.216992  [50001/60000]\n",
            "Train Accuracy: 14.8%\n",
            "Test Error: \n",
            " Accuracy: 14.1%, Avg loss: 6897564356.872132 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 13.701742  [    1/60000]\n",
            "loss: 2.200176  [10001/60000]\n",
            "loss: 2.250309  [20001/60000]\n",
            "loss: 96896.000000  [30001/60000]\n",
            "loss: 2.887661  [40001/60000]\n",
            "loss: 2.039845  [50001/60000]\n",
            "Train Accuracy: 14.9%\n",
            "Test Error: \n",
            " Accuracy: 15.8%, Avg loss: 7384572252.488374 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 100.816650  [    1/60000]\n",
            "loss: 2.167073  [10001/60000]\n",
            "loss: 2.168353  [20001/60000]\n",
            "loss: 183992.000000  [30001/60000]\n",
            "loss: 2.832318  [40001/60000]\n",
            "loss: 2.110111  [50001/60000]\n",
            "Train Accuracy: 15.2%\n",
            "Test Error: \n",
            " Accuracy: 15.7%, Avg loss: 7711892673.306458 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.083970  [10001/60000]\n",
            "loss: 2.226243  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.678335  [40001/60000]\n",
            "loss: 2.101618  [50001/60000]\n",
            "Train Accuracy: 15.5%\n",
            "Test Error: \n",
            " Accuracy: 16.5%, Avg loss: 8439862460.083833 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 46.853882  [    1/60000]\n",
            "loss: 2.100295  [10001/60000]\n",
            "loss: 2.302948  [20001/60000]\n",
            "loss: 2159784.000000  [30001/60000]\n",
            "loss: 2.553613  [40001/60000]\n",
            "loss: 2.135235  [50001/60000]\n",
            "Train Accuracy: 16.0%\n",
            "Test Error: \n",
            " Accuracy: 16.8%, Avg loss: 7410677161.022700 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.106110  [10001/60000]\n",
            "loss: 2.215870  [20001/60000]\n",
            "loss: 5517572.000000  [30001/60000]\n",
            "loss: 2.843355  [40001/60000]\n",
            "loss: 2.044657  [50001/60000]\n",
            "Train Accuracy: 16.2%\n",
            "Test Error: \n",
            " Accuracy: 16.8%, Avg loss: 7492258953.794459 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 27.432173  [    1/60000]\n",
            "loss: 2.045221  [10001/60000]\n",
            "loss: 2.267546  [20001/60000]\n",
            "loss: 2.596118  [30001/60000]\n",
            "loss: 2.734526  [40001/60000]\n",
            "loss: 2.096848  [50001/60000]\n",
            "Train Accuracy: 16.6%\n",
            "Test Error: \n",
            " Accuracy: 17.0%, Avg loss: 8366399875.923209 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 29.979561  [    1/60000]\n",
            "loss: 1.935926  [10001/60000]\n",
            "loss: 2.242908  [20001/60000]\n",
            "loss: 20451408.000000  [30001/60000]\n",
            "loss: 2.445701  [40001/60000]\n",
            "loss: 2.039718  [50001/60000]\n",
            "Train Accuracy: 16.7%\n",
            "Test Error: \n",
            " Accuracy: 17.4%, Avg loss: 7827338150.847422 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.177062  [10001/60000]\n",
            "loss: 2.391905  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.591418  [40001/60000]\n",
            "loss: 2.111334  [50001/60000]\n",
            "Train Accuracy: 17.1%\n",
            "Test Error: \n",
            " Accuracy: 17.4%, Avg loss: 8386030866.521308 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.110538  [10001/60000]\n",
            "loss: 2.343096  [20001/60000]\n",
            "loss: 12492192.000000  [30001/60000]\n",
            "loss: 2.595753  [40001/60000]\n",
            "loss: 2.114913  [50001/60000]\n",
            "Train Accuracy: 17.5%\n",
            "Test Error: \n",
            " Accuracy: 17.6%, Avg loss: 7957019347.782616 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.130963  [10001/60000]\n",
            "loss: 2.346419  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.499442  [40001/60000]\n",
            "loss: 2.120401  [50001/60000]\n",
            "Train Accuracy: 17.6%\n",
            "Test Error: \n",
            " Accuracy: 17.3%, Avg loss: 8151630130.479167 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.019083  [10001/60000]\n",
            "loss: 2.432753  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.534917  [40001/60000]\n",
            "loss: 2.114025  [50001/60000]\n",
            "Train Accuracy: 17.9%\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 8077796195.508131 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.110433  [10001/60000]\n",
            "loss: 2.299744  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.605299  [40001/60000]\n",
            "loss: 2.078966  [50001/60000]\n",
            "Train Accuracy: 18.0%\n",
            "Test Error: \n",
            " Accuracy: 17.3%, Avg loss: 8036618492.793818 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.114916  [10001/60000]\n",
            "loss: 2.390104  [20001/60000]\n",
            "loss: 771936.000000  [30001/60000]\n",
            "loss: 2.517284  [40001/60000]\n",
            "loss: 2.242204  [50001/60000]\n",
            "Train Accuracy: 18.3%\n",
            "Test Error: \n",
            " Accuracy: 17.3%, Avg loss: 8131142124.243705 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.036772  [10001/60000]\n",
            "loss: 2.352600  [20001/60000]\n",
            "loss: 2350328.000000  [30001/60000]\n",
            "loss: 2.461609  [40001/60000]\n",
            "loss: 2.180888  [50001/60000]\n",
            "Train Accuracy: 18.4%\n",
            "Test Error: \n",
            " Accuracy: 17.3%, Avg loss: 8187718169.454523 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.045779  [10001/60000]\n",
            "loss: 2.574615  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.504971  [40001/60000]\n",
            "loss: 2.128574  [50001/60000]\n",
            "Train Accuracy: 18.6%\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 8094640918.636089 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.075217  [10001/60000]\n",
            "loss: 2.489644  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.578734  [40001/60000]\n",
            "loss: 2.219789  [50001/60000]\n",
            "Train Accuracy: 18.7%\n",
            "Test Error: \n",
            " Accuracy: 17.2%, Avg loss: 8048155166.330183 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.054278  [10001/60000]\n",
            "loss: 2.410528  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.479767  [40001/60000]\n",
            "loss: 2.224687  [50001/60000]\n",
            "Train Accuracy: 18.7%\n",
            "Test Error: \n",
            " Accuracy: 17.6%, Avg loss: 8036935471.331980 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.143433  [10001/60000]\n",
            "loss: 2.419371  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.480833  [40001/60000]\n",
            "loss: 2.184018  [50001/60000]\n",
            "Train Accuracy: 18.8%\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 7992713981.461285 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.090413  [10001/60000]\n",
            "loss: 2.486825  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.532856  [40001/60000]\n",
            "loss: 2.279401  [50001/60000]\n",
            "Train Accuracy: 19.0%\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 8001832277.952403 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.111292  [10001/60000]\n",
            "loss: 2.486291  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.429823  [40001/60000]\n",
            "loss: 2.195760  [50001/60000]\n",
            "Train Accuracy: 19.1%\n",
            "Test Error: \n",
            " Accuracy: 17.6%, Avg loss: 7976184720.311922 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.050245  [10001/60000]\n",
            "loss: 2.480013  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.494449  [40001/60000]\n",
            "loss: 2.225515  [50001/60000]\n",
            "Train Accuracy: 19.2%\n",
            "Test Error: \n",
            " Accuracy: 17.6%, Avg loss: 7971364349.107936 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.108198  [10001/60000]\n",
            "loss: 2.499624  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.430863  [40001/60000]\n",
            "loss: 2.190771  [50001/60000]\n",
            "Train Accuracy: 19.3%\n",
            "Test Error: \n",
            " Accuracy: 17.7%, Avg loss: 7963487567.238776 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.118246  [10001/60000]\n",
            "loss: 2.558224  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.483709  [40001/60000]\n",
            "loss: 2.220560  [50001/60000]\n",
            "Train Accuracy: 19.3%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 7961642260.647089 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.138020  [10001/60000]\n",
            "loss: 2.551351  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.464213  [40001/60000]\n",
            "loss: 2.280448  [50001/60000]\n",
            "Train Accuracy: 19.5%\n",
            "Test Error: \n",
            " Accuracy: 17.9%, Avg loss: 7961918026.538715 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.147058  [10001/60000]\n",
            "loss: 2.602389  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.476131  [40001/60000]\n",
            "loss: 2.186649  [50001/60000]\n",
            "Train Accuracy: 19.6%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 7973091273.360765 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.110489  [10001/60000]\n",
            "loss: 2.509298  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.460006  [40001/60000]\n",
            "loss: 2.189920  [50001/60000]\n",
            "Train Accuracy: 19.7%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 7953366225.688785 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.195708  [10001/60000]\n",
            "loss: 2.587108  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.447643  [40001/60000]\n",
            "loss: 2.287313  [50001/60000]\n",
            "Train Accuracy: 19.7%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 7930350445.404831 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.213041  [10001/60000]\n",
            "loss: 2.561992  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.463509  [40001/60000]\n",
            "loss: 2.194282  [50001/60000]\n",
            "Train Accuracy: 19.9%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 7903071331.233817 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.216296  [10001/60000]\n",
            "loss: 2.549623  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.466857  [40001/60000]\n",
            "loss: 2.229237  [50001/60000]\n",
            "Train Accuracy: 19.8%\n",
            "Test Error: \n",
            " Accuracy: 17.9%, Avg loss: 7871390395.379038 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.241715  [10001/60000]\n",
            "loss: 2.636326  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.385283  [40001/60000]\n",
            "loss: 2.174060  [50001/60000]\n",
            "Train Accuracy: 19.9%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 7843221447.585939 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.195357  [10001/60000]\n",
            "loss: 2.598401  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.444247  [40001/60000]\n",
            "loss: 2.176833  [50001/60000]\n",
            "Train Accuracy: 20.0%\n",
            "Test Error: \n",
            " Accuracy: 17.7%, Avg loss: 7820752606.665619 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.199397  [10001/60000]\n",
            "loss: 2.616939  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.495032  [40001/60000]\n",
            "loss: 2.153511  [50001/60000]\n",
            "Train Accuracy: 20.1%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7784515644.234722 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.193113  [10001/60000]\n",
            "loss: 2.587645  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.475708  [40001/60000]\n",
            "loss: 2.209009  [50001/60000]\n",
            "Train Accuracy: 20.1%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7773511485.440219 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.193234  [10001/60000]\n",
            "loss: 2.577173  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.483100  [40001/60000]\n",
            "loss: 2.153223  [50001/60000]\n",
            "Train Accuracy: 20.2%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7748093436.664855 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.189851  [10001/60000]\n",
            "loss: 2.562524  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.486804  [40001/60000]\n",
            "loss: 2.159601  [50001/60000]\n",
            "Train Accuracy: 20.2%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7740454700.792994 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.127964  [10001/60000]\n",
            "loss: 2.595209  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.433843  [40001/60000]\n",
            "loss: 2.196180  [50001/60000]\n",
            "Train Accuracy: 20.2%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 7717704452.590978 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.114237  [10001/60000]\n",
            "loss: 2.606367  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.486445  [40001/60000]\n",
            "loss: 2.162911  [50001/60000]\n",
            "Train Accuracy: 20.3%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 7704388470.044082 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.140783  [10001/60000]\n",
            "loss: 2.586505  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.487370  [40001/60000]\n",
            "loss: 2.184116  [50001/60000]\n",
            "Train Accuracy: 20.2%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7680886780.245420 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.148377  [10001/60000]\n",
            "loss: 2.571702  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.454055  [40001/60000]\n",
            "loss: 2.171517  [50001/60000]\n",
            "Train Accuracy: 20.2%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 7640457353.438907 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.158159  [10001/60000]\n",
            "loss: 2.580940  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.436897  [40001/60000]\n",
            "loss: 2.177934  [50001/60000]\n",
            "Train Accuracy: 20.3%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7631501808.443090 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.120202  [10001/60000]\n",
            "loss: 2.588028  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.448014  [40001/60000]\n",
            "loss: 2.193583  [50001/60000]\n",
            "Train Accuracy: 20.3%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 7619339341.281386 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.164882  [10001/60000]\n",
            "loss: 2.632151  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.439619  [40001/60000]\n",
            "loss: 2.219176  [50001/60000]\n",
            "Train Accuracy: 20.3%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7610988440.452417 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.164924  [10001/60000]\n",
            "loss: 2.627791  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.433718  [40001/60000]\n",
            "loss: 2.175901  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7604063735.627763 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.164089  [10001/60000]\n",
            "loss: 2.658761  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.443424  [40001/60000]\n",
            "loss: 2.158271  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7588486629.100747 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.166251  [10001/60000]\n",
            "loss: 2.660615  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.449396  [40001/60000]\n",
            "loss: 2.169126  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 7584623783.743728 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.160912  [10001/60000]\n",
            "loss: 2.614122  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.469981  [40001/60000]\n",
            "loss: 2.184382  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 17.9%, Avg loss: 7551806063.459391 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.163903  [10001/60000]\n",
            "loss: 2.588497  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.426003  [40001/60000]\n",
            "loss: 2.134106  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7530394104.477356 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.141418  [10001/60000]\n",
            "loss: 2.632416  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.467408  [40001/60000]\n",
            "loss: 2.172986  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7510721694.145373 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.145537  [10001/60000]\n",
            "loss: 2.644874  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.499918  [40001/60000]\n",
            "loss: 2.205683  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7492816776.724289 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.155676  [10001/60000]\n",
            "loss: 2.653282  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.476497  [40001/60000]\n",
            "loss: 2.174724  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7473571458.232855 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.163413  [10001/60000]\n",
            "loss: 2.643582  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.483209  [40001/60000]\n",
            "loss: 2.204696  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7468207279.456377 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.156938  [10001/60000]\n",
            "loss: 2.650641  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.471425  [40001/60000]\n",
            "loss: 2.214773  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 17.9%, Avg loss: 7450300307.469247 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.168001  [10001/60000]\n",
            "loss: 2.671885  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.487783  [40001/60000]\n",
            "loss: 2.217349  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 7440936192.968834 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.152702  [10001/60000]\n",
            "loss: 2.630576  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.484718  [40001/60000]\n",
            "loss: 2.219979  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7428929620.759503 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.176270  [10001/60000]\n",
            "loss: 2.672617  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.440084  [40001/60000]\n",
            "loss: 2.217783  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7422895911.368615 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.189144  [10001/60000]\n",
            "loss: 2.653027  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.476438  [40001/60000]\n",
            "loss: 2.202036  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 7417947822.523208 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.166011  [10001/60000]\n",
            "loss: 2.675628  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.443323  [40001/60000]\n",
            "loss: 2.215534  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7410531484.426794 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.185848  [10001/60000]\n",
            "loss: 2.678975  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.436858  [40001/60000]\n",
            "loss: 2.218534  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7400239865.886209 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.179528  [10001/60000]\n",
            "loss: 2.632594  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.428985  [40001/60000]\n",
            "loss: 2.230609  [50001/60000]\n",
            "Train Accuracy: 20.6%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7405353900.518942 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.195600  [10001/60000]\n",
            "loss: 2.649333  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.438803  [40001/60000]\n",
            "loss: 2.222363  [50001/60000]\n",
            "Train Accuracy: 20.6%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7395614643.555784 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.180482  [10001/60000]\n",
            "loss: 2.663937  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.408164  [40001/60000]\n",
            "loss: 2.224602  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7397047945.838864 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.188757  [10001/60000]\n",
            "loss: 2.664127  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.395375  [40001/60000]\n",
            "loss: 2.222132  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7394300263.654447 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.194314  [10001/60000]\n",
            "loss: 2.669446  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.402915  [40001/60000]\n",
            "loss: 2.194857  [50001/60000]\n",
            "Train Accuracy: 20.6%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7401273530.770742 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.186583  [10001/60000]\n",
            "loss: 2.673443  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.416399  [40001/60000]\n",
            "loss: 2.214549  [50001/60000]\n",
            "Train Accuracy: 20.7%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7403652620.001388 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.200310  [10001/60000]\n",
            "loss: 2.670267  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.421052  [40001/60000]\n",
            "loss: 2.210906  [50001/60000]\n",
            "Train Accuracy: 20.7%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7397914033.694567 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.187119  [10001/60000]\n",
            "loss: 2.682958  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.399690  [40001/60000]\n",
            "loss: 2.213815  [50001/60000]\n",
            "Train Accuracy: 20.8%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7401534636.253439 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.196595  [10001/60000]\n",
            "loss: 2.682983  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.413692  [40001/60000]\n",
            "loss: 2.224543  [50001/60000]\n",
            "Train Accuracy: 20.7%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7399964548.004630 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.195013  [10001/60000]\n",
            "loss: 2.660001  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.408442  [40001/60000]\n",
            "loss: 2.222180  [50001/60000]\n",
            "Train Accuracy: 20.8%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7388740128.567696 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.189536  [10001/60000]\n",
            "loss: 2.658341  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.406787  [40001/60000]\n",
            "loss: 2.223705  [50001/60000]\n",
            "Train Accuracy: 20.9%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7377415834.887446 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.183247  [10001/60000]\n",
            "loss: 2.669202  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.417471  [40001/60000]\n",
            "loss: 2.203868  [50001/60000]\n",
            "Train Accuracy: 20.9%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7354057527.960747 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.181856  [10001/60000]\n",
            "loss: 2.687323  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.429641  [40001/60000]\n",
            "loss: 2.202590  [50001/60000]\n",
            "Train Accuracy: 20.9%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7328539428.960854 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.175537  [10001/60000]\n",
            "loss: 2.700566  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.414420  [40001/60000]\n",
            "loss: 2.198512  [50001/60000]\n",
            "Train Accuracy: 20.9%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7317833055.443873 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.168929  [10001/60000]\n",
            "loss: 2.702816  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.409997  [40001/60000]\n",
            "loss: 2.211479  [50001/60000]\n",
            "Train Accuracy: 20.9%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7301185059.563964 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.183381  [10001/60000]\n",
            "loss: 2.711739  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.413445  [40001/60000]\n",
            "loss: 2.217356  [50001/60000]\n",
            "Train Accuracy: 21.0%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 7287545040.473652 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.188037  [10001/60000]\n",
            "loss: 2.714257  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.404835  [40001/60000]\n",
            "loss: 2.216551  [50001/60000]\n",
            "Train Accuracy: 21.0%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7275114672.240245 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.193737  [10001/60000]\n",
            "loss: 2.693433  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.414741  [40001/60000]\n",
            "loss: 2.220407  [50001/60000]\n",
            "Train Accuracy: 21.1%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7262865475.906014 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.205128  [10001/60000]\n",
            "loss: 2.661718  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.415661  [40001/60000]\n",
            "loss: 2.228157  [50001/60000]\n",
            "Train Accuracy: 21.2%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 7236370088.155758 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.199614  [10001/60000]\n",
            "loss: 2.657338  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.415458  [40001/60000]\n",
            "loss: 2.242189  [50001/60000]\n",
            "Train Accuracy: 21.2%\n",
            "Test Error: \n",
            " Accuracy: 18.4%, Avg loss: 7209531030.973662 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.208829  [10001/60000]\n",
            "loss: 2.660109  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.398856  [40001/60000]\n",
            "loss: 2.230818  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.4%, Avg loss: 7185813995.560934 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.209584  [10001/60000]\n",
            "loss: 2.671186  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.388765  [40001/60000]\n",
            "loss: 2.239758  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.4%, Avg loss: 7162489945.946550 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.207916  [10001/60000]\n",
            "loss: 2.660611  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.380316  [40001/60000]\n",
            "loss: 2.220324  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 7132742955.162427 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.193826  [10001/60000]\n",
            "loss: 2.663013  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.383554  [40001/60000]\n",
            "loss: 2.230429  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 7096510627.175603 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.200093  [10001/60000]\n",
            "loss: 2.670528  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.387587  [40001/60000]\n",
            "loss: 2.209641  [50001/60000]\n",
            "Train Accuracy: 21.4%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 7061225784.490250 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.191398  [10001/60000]\n",
            "loss: 2.678109  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.388383  [40001/60000]\n",
            "loss: 2.208118  [50001/60000]\n",
            "Train Accuracy: 21.4%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 7025739028.618957 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.189782  [10001/60000]\n",
            "loss: 2.680805  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.387648  [40001/60000]\n",
            "loss: 2.214255  [50001/60000]\n",
            "Train Accuracy: 21.4%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 6989318123.629635 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.196266  [10001/60000]\n",
            "loss: 2.672556  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.386135  [40001/60000]\n",
            "loss: 2.218770  [50001/60000]\n",
            "Train Accuracy: 21.4%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6955826973.652471 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.198094  [10001/60000]\n",
            "loss: 2.657619  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.395092  [40001/60000]\n",
            "loss: 2.219389  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6924688808.636998 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.200072  [10001/60000]\n",
            "loss: 2.649180  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.382985  [40001/60000]\n",
            "loss: 2.222142  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6895729908.790201 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.199563  [10001/60000]\n",
            "loss: 2.651146  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.394691  [40001/60000]\n",
            "loss: 2.217801  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 6870797539.935221 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.200679  [10001/60000]\n",
            "loss: 2.637718  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.395466  [40001/60000]\n",
            "loss: 2.230409  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 6846253098.974292 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.211944  [10001/60000]\n",
            "loss: 2.624282  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.392241  [40001/60000]\n",
            "loss: 2.231457  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.7%, Avg loss: 6812216445.406553 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.212666  [10001/60000]\n",
            "loss: 2.620320  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.390615  [40001/60000]\n",
            "loss: 2.229321  [50001/60000]\n",
            "Train Accuracy: 21.4%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 6776250548.830204 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.213093  [10001/60000]\n",
            "loss: 2.619137  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.382988  [40001/60000]\n",
            "loss: 2.234231  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6743494306.279254 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.221831  [10001/60000]\n",
            "loss: 2.618153  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.379703  [40001/60000]\n",
            "loss: 2.230688  [50001/60000]\n",
            "Train Accuracy: 21.4%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 6712294535.631857 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.220529  [10001/60000]\n",
            "loss: 2.605400  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.383256  [40001/60000]\n",
            "loss: 2.234355  [50001/60000]\n",
            "Train Accuracy: 21.5%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6683867664.426220 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.226573  [10001/60000]\n",
            "loss: 2.606985  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.383740  [40001/60000]\n",
            "loss: 2.238428  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.6%, Avg loss: 6655010792.333364 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.228866  [10001/60000]\n",
            "loss: 2.600735  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.390133  [40001/60000]\n",
            "loss: 2.233484  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6627761345.906560 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.223266  [10001/60000]\n",
            "loss: 2.592318  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.389110  [40001/60000]\n",
            "loss: 2.231558  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6601624111.284593 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.226745  [10001/60000]\n",
            "loss: 2.589822  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.383801  [40001/60000]\n",
            "loss: 2.237778  [50001/60000]\n",
            "Train Accuracy: 21.2%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6571962770.681197 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.226116  [10001/60000]\n",
            "loss: 2.582452  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.383224  [40001/60000]\n",
            "loss: 2.239295  [50001/60000]\n",
            "Train Accuracy: 21.2%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6544399629.818942 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.227593  [10001/60000]\n",
            "loss: 2.578861  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.380914  [40001/60000]\n",
            "loss: 2.231864  [50001/60000]\n",
            "Train Accuracy: 21.3%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6514605400.849510 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.225721  [10001/60000]\n",
            "loss: 2.579439  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.380685  [40001/60000]\n",
            "loss: 2.232098  [50001/60000]\n",
            "Train Accuracy: 21.2%\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 6489255238.864347 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.228626  [10001/60000]\n",
            "loss: 2.574947  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.381546  [40001/60000]\n",
            "loss: 2.231487  [50001/60000]\n",
            "Train Accuracy: 21.1%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 6464267070.917827 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.227579  [10001/60000]\n",
            "loss: 2.570709  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.379060  [40001/60000]\n",
            "loss: 2.233452  [50001/60000]\n",
            "Train Accuracy: 21.0%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6438332998.441207 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.225509  [10001/60000]\n",
            "loss: 2.568485  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.378962  [40001/60000]\n",
            "loss: 2.229398  [50001/60000]\n",
            "Train Accuracy: 21.0%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 6410658520.600768 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.224449  [10001/60000]\n",
            "loss: 2.561901  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.377553  [40001/60000]\n",
            "loss: 2.229490  [50001/60000]\n",
            "Train Accuracy: 20.9%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 6383565876.716643 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.226726  [10001/60000]\n",
            "loss: 2.553067  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.374084  [40001/60000]\n",
            "loss: 2.232373  [50001/60000]\n",
            "Train Accuracy: 20.8%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6357737568.110955 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.227576  [10001/60000]\n",
            "loss: 2.550677  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.373984  [40001/60000]\n",
            "loss: 2.231685  [50001/60000]\n",
            "Train Accuracy: 20.7%\n",
            "Test Error: \n",
            " Accuracy: 18.3%, Avg loss: 6333900385.196822 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.225317  [10001/60000]\n",
            "loss: 2.545019  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.369021  [40001/60000]\n",
            "loss: 2.227401  [50001/60000]\n",
            "Train Accuracy: 20.6%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6312045690.873006 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.221971  [10001/60000]\n",
            "loss: 2.540993  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.366122  [40001/60000]\n",
            "loss: 2.225633  [50001/60000]\n",
            "Train Accuracy: 20.6%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6292041295.083238 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.220799  [10001/60000]\n",
            "loss: 2.536231  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.362265  [40001/60000]\n",
            "loss: 2.224191  [50001/60000]\n",
            "Train Accuracy: 20.5%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6272487422.497290 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.218539  [10001/60000]\n",
            "loss: 2.529025  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.358674  [40001/60000]\n",
            "loss: 2.223814  [50001/60000]\n",
            "Train Accuracy: 20.4%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 6252611384.827043 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.219678  [10001/60000]\n",
            "loss: 2.526086  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.358922  [40001/60000]\n",
            "loss: 2.224327  [50001/60000]\n",
            "Train Accuracy: 20.3%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 6232797488.712496 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.218985  [10001/60000]\n",
            "loss: 2.523082  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.355839  [40001/60000]\n",
            "loss: 2.222329  [50001/60000]\n",
            "Train Accuracy: 20.2%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6215171676.877192 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.217571  [10001/60000]\n",
            "loss: 2.523761  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.354005  [40001/60000]\n",
            "loss: 2.220073  [50001/60000]\n",
            "Train Accuracy: 20.1%\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 6197719160.966436 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.216384  [10001/60000]\n",
            "loss: 2.522823  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.351984  [40001/60000]\n",
            "loss: 2.221042  [50001/60000]\n",
            "Train Accuracy: 20.0%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 6181311379.100800 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.218827  [10001/60000]\n",
            "loss: 2.520642  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.350812  [40001/60000]\n",
            "loss: 2.221528  [50001/60000]\n",
            "Train Accuracy: 19.9%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 6164992743.533592 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.217278  [10001/60000]\n",
            "loss: 2.518127  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.347458  [40001/60000]\n",
            "loss: 2.222012  [50001/60000]\n",
            "Train Accuracy: 19.9%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 6149886245.869607 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.219746  [10001/60000]\n",
            "loss: 2.517576  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.345877  [40001/60000]\n",
            "loss: 2.224158  [50001/60000]\n",
            "Train Accuracy: 19.7%\n",
            "Test Error: \n",
            " Accuracy: 18.1%, Avg loss: 6135233545.917508 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.220785  [10001/60000]\n",
            "loss: 2.514431  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.345876  [40001/60000]\n",
            "loss: 2.222404  [50001/60000]\n",
            "Train Accuracy: 19.7%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 6120191252.990586 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.220840  [10001/60000]\n",
            "loss: 2.511454  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.343250  [40001/60000]\n",
            "loss: 2.222458  [50001/60000]\n",
            "Train Accuracy: 19.6%\n",
            "Test Error: \n",
            " Accuracy: 18.0%, Avg loss: 6106267976.283407 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.220700  [10001/60000]\n",
            "loss: 2.510574  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.341870  [40001/60000]\n",
            "loss: 2.223813  [50001/60000]\n",
            "Train Accuracy: 19.5%\n",
            "Test Error: \n",
            " Accuracy: 17.9%, Avg loss: 6091597927.814231 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.222227  [10001/60000]\n",
            "loss: 2.511341  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.341115  [40001/60000]\n",
            "loss: 2.224473  [50001/60000]\n",
            "Train Accuracy: 19.4%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 6077676305.833759 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.221969  [10001/60000]\n",
            "loss: 2.511242  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.338647  [40001/60000]\n",
            "loss: 2.224065  [50001/60000]\n",
            "Train Accuracy: 19.4%\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 6064269857.179320 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.221717  [10001/60000]\n",
            "loss: 2.510310  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.336824  [40001/60000]\n",
            "loss: 2.222949  [50001/60000]\n",
            "Train Accuracy: 19.3%\n",
            "Test Error: \n",
            " Accuracy: 17.7%, Avg loss: 6051816987.802444 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.220876  [10001/60000]\n",
            "loss: 2.510898  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.335524  [40001/60000]\n",
            "loss: 2.223488  [50001/60000]\n",
            "Train Accuracy: 19.2%\n",
            "Test Error: \n",
            " Accuracy: 17.7%, Avg loss: 6039604664.991989 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.222562  [10001/60000]\n",
            "loss: 2.511184  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.333663  [40001/60000]\n",
            "loss: 2.224818  [50001/60000]\n",
            "Train Accuracy: 19.2%\n",
            "Test Error: \n",
            " Accuracy: 17.6%, Avg loss: 6028181433.513794 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.223245  [10001/60000]\n",
            "loss: 2.510600  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.333194  [40001/60000]\n",
            "loss: 2.225258  [50001/60000]\n",
            "Train Accuracy: 19.1%\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 6017296554.605618 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss: 0.000000  [    1/60000]\n",
            "loss: 2.223676  [10001/60000]\n",
            "loss: 2.510968  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.331987  [40001/60000]\n",
            "loss: 2.225483  [50001/60000]\n",
            "Train Accuracy: 19.0%\n",
            "Test Error: \n",
            " Accuracy: 17.4%, Avg loss: 6007002651.127255 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss: 17.276976  [    1/60000]\n",
            "loss: 2.223678  [10001/60000]\n",
            "loss: 2.509741  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.331691  [40001/60000]\n",
            "loss: 2.225915  [50001/60000]\n",
            "Train Accuracy: 19.0%\n",
            "Test Error: \n",
            " Accuracy: 17.4%, Avg loss: 5996992395.536816 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss: 16.391806  [    1/60000]\n",
            "loss: 2.224735  [10001/60000]\n",
            "loss: 2.510005  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.331465  [40001/60000]\n",
            "loss: 2.227363  [50001/60000]\n",
            "Train Accuracy: 18.8%\n",
            "Test Error: \n",
            " Accuracy: 17.4%, Avg loss: 5987832282.599830 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss: 15.585011  [    1/60000]\n",
            "loss: 2.225924  [10001/60000]\n",
            "loss: 2.510183  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.330508  [40001/60000]\n",
            "loss: 2.228365  [50001/60000]\n",
            "Train Accuracy: 18.8%\n",
            "Test Error: \n",
            " Accuracy: 17.4%, Avg loss: 5978968164.246102 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss: 14.822865  [    1/60000]\n",
            "loss: 2.227788  [10001/60000]\n",
            "loss: 2.510558  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.328527  [40001/60000]\n",
            "loss: 2.229383  [50001/60000]\n",
            "Train Accuracy: 18.7%\n",
            "Test Error: \n",
            " Accuracy: 17.3%, Avg loss: 5970779312.048077 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss: 14.100639  [    1/60000]\n",
            "loss: 2.228010  [10001/60000]\n",
            "loss: 2.511000  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.327013  [40001/60000]\n",
            "loss: 2.230239  [50001/60000]\n",
            "Train Accuracy: 18.6%\n",
            "Test Error: \n",
            " Accuracy: 17.2%, Avg loss: 5962927148.383890 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss: 13.429932  [    1/60000]\n",
            "loss: 2.229347  [10001/60000]\n",
            "loss: 2.510429  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.326303  [40001/60000]\n",
            "loss: 2.231548  [50001/60000]\n",
            "Train Accuracy: 18.5%\n",
            "Test Error: \n",
            " Accuracy: 17.3%, Avg loss: 5955461206.051366 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss: 12.733173  [    1/60000]\n",
            "loss: 2.230582  [10001/60000]\n",
            "loss: 2.510530  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.324549  [40001/60000]\n",
            "loss: 2.232469  [50001/60000]\n",
            "Train Accuracy: 18.4%\n",
            "Test Error: \n",
            " Accuracy: 17.2%, Avg loss: 5948602428.503353 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss: 12.108591  [    1/60000]\n",
            "loss: 2.231711  [10001/60000]\n",
            "loss: 2.511402  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.323917  [40001/60000]\n",
            "loss: 2.233473  [50001/60000]\n",
            "Train Accuracy: 18.4%\n",
            "Test Error: \n",
            " Accuracy: 17.2%, Avg loss: 5941888890.535974 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "loss: 11.548044  [    1/60000]\n",
            "loss: 2.232696  [10001/60000]\n",
            "loss: 2.511173  [20001/60000]\n",
            "loss: 0.000000  [30001/60000]\n",
            "loss: 2.322594  [40001/60000]\n",
            "loss: 2.234711  [50001/60000]\n",
            "Train Accuracy: 18.3%\n",
            "Test Error: \n",
            " Accuracy: 17.2%, Avg loss: 5935476705.934563 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "load_from = -1\n",
        "# Define a directory to save the model weights\n",
        "drive_save_dir = '/content/drive/My Drive/HebbNet_model_weights/'\n",
        "\n",
        "# Load the model architecture\n",
        "model = HebbNet().to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD([\n",
        "            {'params': model.classification_weights.parameters(), 'lr': lr, 'momentum': momentum} ])\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=1, gamma=lr_decay)\n",
        "activation_thresholder = HebbRuleWithActivationThreshold().to(device)\n",
        "\n",
        "# Load the most recent model weights if available\n",
        "if load_from > -1:\n",
        "  checkpoint_path = os.path.join(drive_save_dir, f'model_epoch_{load_from}.pth')\n",
        "  model, activation_thresholder, optimizer, scheduler = load_model(model, optimizer, scheduler, checkpoint_path)\n",
        "\n",
        "\n",
        "for t in range(load_from+1 ,epochs):\n",
        "    print(f\"Epoch {t}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, scheduler.get_last_lr()[0], activation_thresholder)\n",
        "    # learning rate decsy for the hebbian layer\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    scheduler.step()\n",
        "    # Save the model weights after each epoch\n",
        "    # save_model(model, activation_thresholder, optimizer, scheduler, os.path.join(drive_save_dir, f\"model_epoch_{t}.pth\"))\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czPTdFY9UOY0"
      },
      "source": [
        "Viaualize the hebbian weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BLP3yz5UMTI"
      },
      "outputs": [],
      "source": [
        "#visualize_weights(model.hebbian_weights.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kA3QMGnTdDU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "0bf02749-20ea-4c88-a854-f7cecd9849bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-054043e9e029>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "X = next(iter(test_dataloader))[0].to(device)\n",
        "X,z_hidden,pred = model(X)\n",
        "delta_w1 = activation_thresholder(X, z_hidden).squeeze()\n",
        "# delta_w1 = gradiant_sparsity(delta_w1, p)\n",
        "delta_w1 = delta_w1.reshape(2000,28,28)\n",
        "for d_w in delta_w1:\n",
        "  plt.imshow(d_w.detach().cpu().numpy(), cmap='gray')\n",
        "  plt.show()\n",
        "  '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hlhaV67pm8X"
      },
      "outputs": [],
      "source": [
        "# Directory containing your .pth files\n",
        "drive_save_dir = '/content/drive/My Drive/HebbNet_model_weights/'\n",
        "\n",
        "# Lists to store epoch and accuracy data\n",
        "epochs = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Loop through .pth files\n",
        "for filename in os.listdir(drive_save_dir):\n",
        "    if filename.startswith(\"model_epoch_\") and filename.endswith(\".pth\"):\n",
        "        epoch = int(filename.split(\"_\")[-1].split(\".\")[0])\n",
        "        checkpoint_path = os.path.join(drive_save_dir, f'model_epoch_{epoch}.pth')\n",
        "        model, _, _, _ = load_model(model, optimizer, scheduler, checkpoint_path)\n",
        "        print(epoch)\n",
        "        train_accuracy = test(train_dataloader,model, loss_fn)\n",
        "        test_accuracy = test(test_dataloader, model, loss_fn)\n",
        "        epochs.append(epoch)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Sort the data by epoch number\n",
        "# epochs, accuracies = zip(*sorted(zip(epochs, accuracies)))\n",
        "\n",
        "# Create a figure for accuracy vs. epoch\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, test_accuracies, marker='o', linestyle='-')\n",
        "plt.title('Test Accuracy vs. Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracies, marker='o', linestyle='-')\n",
        "plt.title('Train Accuracy vs. Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwXb5y_yqA4s"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_data = torch.cat([data for data, _ in train_dataloader], dim=0).to(device)\n",
        "all_labels = torch.cat([labels for _, labels in train_dataloader], dim=0).to(device)\n",
        "print(all_data.shape)\n",
        "print(all_labels.shape)\n",
        "\n",
        "X,z_hidden,pred = model(all_data)\n",
        "\n",
        "print(X.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for one hots PCA Kmeans - doesnt achive good clustering"
      ],
      "metadata": {
        "id": "aK-Vy0pqDKLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_components = 2  # You can change this number\n",
        "# Generate one-hot encoded vectors and matching labels\n",
        "vectors = []\n",
        "labels = []\n",
        "for _ in range(100):\n",
        "    # Randomly choose the index for the '1' in the one-hot vector\n",
        "    one_index = np.random.randint(10)\n",
        "\n",
        "    # Create a one-hot vector with one '1' at the chosen index\n",
        "    vector = np.zeros(10)\n",
        "    vector[one_index] = 1\n",
        "\n",
        "    # Append the vector to the list of vectors\n",
        "    vectors.append(vector)\n",
        "\n",
        "    # Append the label (the index where '1' is located) to the list of labels\n",
        "    labels.append(one_index)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(vectors)\n",
        "\n",
        "k = 10  # Number of clusters\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Create a scatter plot of the PCA results with colored labels\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='rainbow')\n",
        "plt.title('K-means Clusters')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "# Assuming X is your data and y_true is the true labels\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='rainbow')\n",
        "plt.title('True Labels')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KY2o1JHOAukS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPT3+uBwWeQEasQqUwM7F0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}